# Modeling the DC Area Real Estate Market
Walter Tyrna

## Abstract
The goal of this project is to create a model that can determine the most important factors that drive a given property's final sale price. The model also should reasonably predict a property's final sale price.
This information would help home sellers and investors determine how to best match asking price with a property's potential sales price. 

I used data scraped from Compass (https://www.compass.com/), a major DC-area real estate company, for real estate listings in conjunction with some information scraped from DC Metro's website (https://www.wmata.com/) to develop models using Linear Regression and Lasso modeling techniques. I used tables generated by Pandas and Folium to communicate my results.

## Design
This project considered features that relate to a property's physical state (such as beds/baths, stories, etc) as well as physical location (in terms of zipcode/general area as well as distance from downtown and metro stations).

Identifying features that most impact a property's ultimate sales price can not only help a seller adequately determine an sales price, but also determine what physical features can be improved (or left alone) to improve a property's sales price. For example, whether or not adding a bedroom or building a garage would be worth the investment. 


## Data
The original dataset included over 2,500 real estate listings with 15 inital features (6 categorical). To inform the distance-related features, coordinates for the DC area's 91 metro stations were included.
These 2,500 listings were scoped to include only single-family/townhomes within 25mi of downtown DC, yeilding under 1,000 listings for analysis. The listing's features were down-selected during the feature engineering process.

## Algorithms
*EDA*
1. Cleaning scraped data for nulls or irrelevant data.
2. Identifying and removing outliers.
3. Determining geographic information for each property.

*Feature Engineering*
1.  Creating correlation heatmap using Seaborn to determine collinearity among features & removing colinear features.
2.  Converting categorical features to binary dummy variables; removing features that act as binary (yes, no)
3.  Creating secondary correlation heatmap with categorial features to further downselect
4.  Determining the correlation of each feature with the target (Sold Price), downselecting features to only those with an absoulte value of > 0.1

*Models*
  
This project used Linear regression and Lasso regression in order to maintain model interpretability. Both models used K-fold to improve R2 confidence. 

*Model Evaluation and Selection*
  
The entire training dataset of 59,400 records was split into 80/20 train vs. holdout, and all scores reported below were calculated with 5-fold cross validation on the training portion only. Predictions on the 20% holdout were limited to the very end, so this split was only used and scores seen just once.

The official metric for DrivenData was classification rate (accuracy); however, class weights were included to improve performance against F1 score and provide a more useful real-world application where classification of the minority class (functional needs repair) would be essential.

**Linear Regression Scores:** 
   - Train R2: 0.8579
   - Test R2: 0.8617

**Lasso Regression Scores:** 
   - Train R2: 0.8580
   - Test R2: 0.8622

## Tools
- BeautifulSoup for web scraping
- Numpy and Pandas for data manipulation
- Scikit-learn for modeling
- Matplotlib, Seaborn, Plotly for plotting & visualizations
- Geopy and Folium for geographic data & visualizations
- TQDM for function validation

## Communication
Slides and visuals presented
